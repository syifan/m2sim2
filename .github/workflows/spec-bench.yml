name: SPEC Benchmark Daily

# SPEC CPU 2017 benchmark validation
# Runs daily on a self-hosted runner with SPEC installed
# Not blocking PRs - informational only

on:
  workflow_dispatch:
    inputs:
      benchmark:
        description: 'Specific benchmark to run (blank = all available)'
        type: choice
        required: false
        default: ''
        options:
          - ''
          - '548.exchange2_r'
          - '557.xz_r'
          - '505.mcf_r'
          - '531.deepsjeng_r'
      report_issue:
        description: 'Issue number to comment results on'
        type: string
        required: false
        default: ''

# Only one SPEC run at a time
concurrency:
  group: spec-bench
  cancel-in-progress: false

jobs:
  spec-bench:
    name: SPEC Benchmark Validation
    # Requires self-hosted runner with SPEC installed
    # Label: macos-arm64-spec (or macos-14 for testing without SPEC)
    runs-on: macos-14
    timeout-minutes: 480  # 8 hours max for full SPEC runs

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.25'

      - name: Install Ginkgo
        run: go install github.com/onsi/ginkgo/v2/ginkgo@latest

      - name: Check SPEC availability
        id: spec-check
        run: |
          SPEC_PATH="benchmarks/spec"
          if [ -L "$SPEC_PATH" ] && [ -d "$(readlink "$SPEC_PATH")" ]; then
            echo "spec_available=true" >> $GITHUB_OUTPUT
            echo "✅ SPEC installation found"
            
            # Check for built binaries
            AVAILABLE=$(go run ./cmd/spec-check 2>/dev/null || echo "0")
            echo "available_benchmarks=$AVAILABLE" >> $GITHUB_OUTPUT
          else
            echo "spec_available=false" >> $GITHUB_OUTPUT
            echo "⚠️ SPEC not found at $SPEC_PATH"
            echo "This workflow requires a self-hosted runner with SPEC installed."
          fi

      - name: Run SPEC benchmarks
        id: spec-run
        if: steps.spec-check.outputs.spec_available == 'true'
        run: |
          echo "Running SPEC benchmark validation..."
          mkdir -p results/spec

          # Determine which benchmarks to run
          BENCHMARK_FILTER=""
          if [ -n "${{ inputs.benchmark }}" ]; then
            BENCHMARK_FILTER="-run TestSPEC.*${{ inputs.benchmark }}"
            echo "Running specific benchmark: ${{ inputs.benchmark }}"
          else
            BENCHMARK_FILTER="-run TestSPEC"
            echo "Running all available SPEC benchmarks"
          fi

          # Run SPEC tests with extended timeout and result capture
          if go test -v -timeout 7h ./benchmarks/... $BENCHMARK_FILTER 2>&1 | tee results/spec/execution-log.txt; then
            echo "spec_passed=true" >> $GITHUB_OUTPUT
            echo "✅ SPEC benchmark execution completed successfully"
          else
            echo "⚠️ Some SPEC tests failed or timed out"
            echo "spec_passed=false" >> $GITHUB_OUTPUT
          fi

          # Extract timing results if available
          if grep -q "timing:" results/spec/execution-log.txt; then
            grep "timing:" results/spec/execution-log.txt > results/spec/timing-summary.txt
            echo "Timing data extracted"
          fi

      - name: Generate SPEC accuracy report
        if: steps.spec-check.outputs.spec_available == 'true'
        run: |
          mkdir -p reports/spec
          REPORT_DATE=$(date -u +"%Y-%m-%d %H:%M UTC")
          SPEC_STATUS="${{ steps.spec-run.outputs.spec_passed }}"
          BENCHMARK="${{ inputs.benchmark }}"
          if [ -z "$BENCHMARK" ]; then
            BENCHMARK="all"
          fi

          cat > reports/spec/spec-report.md << EOF
          # SPEC Benchmark Report

          Generated: ${REPORT_DATE}
          Run ID: ${{ github.run_number }}

          ## Configuration

          - **Benchmark:** ${BENCHMARK}
          - **Runner:** ${{ runner.os }} / ${{ runner.arch }}
          - **Timeout:** 8 hours
          - **Available benchmarks:** 548.exchange2_r, 557.xz_r, 505.mcf_r, 531.deepsjeng_r

          ## Status

          Tests passed: ${SPEC_STATUS}

          ## Results

          EOF

          # Add timing summary if available
          if [ -f "results/spec/timing-summary.txt" ]; then
            echo "" >> reports/spec/spec-report.md
            echo "### Timing Summary" >> reports/spec/spec-report.md
            echo "" >> reports/spec/spec-report.md
            echo "\`\`\`" >> reports/spec/spec-report.md
            cat results/spec/timing-summary.txt >> reports/spec/spec-report.md
            echo "\`\`\`" >> reports/spec/spec-report.md
          fi

          # Add execution logs summary
          echo "" >> reports/spec/spec-report.md
          echo "### Execution Summary" >> reports/spec/spec-report.md
          echo "" >> reports/spec/spec-report.md
          if [ -f "results/spec/execution-log.txt" ]; then
            echo "See execution-log.txt in artifacts for full details." >> reports/spec/spec-report.md
            echo "" >> reports/spec/spec-report.md
            echo "**Exit codes:**" >> reports/spec/spec-report.md
            grep -i "exit\|passed\|failed" results/spec/execution-log.txt | tail -10 >> reports/spec/spec-report.md || echo "No exit codes found" >> reports/spec/spec-report.md
          fi

          echo "Report generated at reports/spec/spec-report.md"

      - name: Upload SPEC Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: spec-report-${{ github.run_number }}
          path: |
            reports/spec/
            results/spec/
          retention-days: 90

      - name: Comment on tracking issue
        if: inputs.report_issue != '' && always()
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          ISSUE_NUM="${{ inputs.report_issue }}"
          SPEC_STATUS="${{ steps.spec-run.outputs.spec_passed }}"
          BENCHMARK="${{ inputs.benchmark }}"
          if [ -z "$BENCHMARK" ]; then
            BENCHMARK="all available"
          fi

          # Create issue comment
          cat > comment.md << EOF
          # [Diana] SPEC Benchmark Execution Report

          **Run ID:** ${{ github.run_number }}
          **Date:** $(date -u +"%Y-%m-%d %H:%M UTC")
          **Benchmark:** ${BENCHMARK}
          **Status:** ${SPEC_STATUS}

          EOF

          if [ "${{ steps.spec-check.outputs.spec_available }}" != "true" ]; then
            cat >> comment.md << EOF
          ⚠️ **SPEC not available on runner.** Need self-hosted runner with SPEC installed.
          EOF
          elif [ "${SPEC_STATUS}" = "true" ]; then
            cat >> comment.md << EOF
          ✅ **SPEC benchmark execution completed successfully.**

          See [workflow artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}) for detailed results.
          EOF
          else
            cat >> comment.md << EOF
          ❌ **SPEC benchmark execution failed or timed out.**

          See [workflow logs](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}) for failure details.
          EOF
          fi

          # Post comment to issue
          gh issue comment "$ISSUE_NUM" --body-file comment.md

      - name: Post Summary
        if: always()
        run: |
          echo "## SPEC Benchmark Daily Run" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Date:** $(date -u +"%Y-%m-%d %H:%M UTC")" >> $GITHUB_STEP_SUMMARY
          echo "- **Benchmark:** ${{ inputs.benchmark || 'all available' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **SPEC Available:** ${{ steps.spec-check.outputs.spec_available }}" >> $GITHUB_STEP_SUMMARY

          if [ "${{ steps.spec-check.outputs.spec_available }}" != "true" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "⚠️ **SPEC not installed on this runner.**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "To enable SPEC benchmarks:" >> $GITHUB_STEP_SUMMARY
            echo "1. Use a self-hosted runner with SPEC CPU 2017 installed" >> $GITHUB_STEP_SUMMARY
            echo "2. Run \`scripts/spec-setup.sh link\` to create the symlink" >> $GITHUB_STEP_SUMMARY
            echo "3. Run \`scripts/spec-setup.sh build\` to compile ARM64 binaries" >> $GITHUB_STEP_SUMMARY
          else
            SPEC_PASSED="${{ steps.spec-run.outputs.spec_passed }}"
            echo "- **Tests Passed:** ${SPEC_PASSED}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            if [ "${SPEC_PASSED}" != "true" ]; then
              echo "⚠️ **Some SPEC tests failed.** Check workflow logs for details." >> $GITHUB_STEP_SUMMARY
            else
              echo "✅ All SPEC tests passed." >> $GITHUB_STEP_SUMMARY
            fi
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "See artifacts for detailed benchmark results." >> $GITHUB_STEP_SUMMARY
          fi
