name: CI Metrics Dashboard

on:
  schedule:
    - cron: '0 8 * * *'  # Daily at 8 AM UTC
  workflow_dispatch:

jobs:
  generate-ci-dashboard:
    name: Generate CI Metrics Dashboard
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install Python dependencies
        run: |
          pip install matplotlib numpy pandas requests plotly kaleido

      - name: Generate comprehensive CI metrics
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python3 - <<'PYEOF'
          import json
          import subprocess
          import pandas as pd
          import matplotlib.pyplot as plt
          import plotly.graph_objects as go
          import plotly.express as px
          from plotly.subplots import make_subplots
          from datetime import datetime, timedelta
          import numpy as np

          def get_workflow_runs(workflow_name, days=30):
              """Get workflow runs for the past N days"""
              cmd = ['gh', 'run', 'list', '--workflow', workflow_name, '--json',
                     'status,conclusion,createdAt,updatedAt,runNumber,id,name', '--limit', '100']
              result = subprocess.run(cmd, capture_output=True, text=True)

              if result.returncode != 0:
                  print(f"Warning: Could not fetch runs for {workflow_name}")
                  return []

              runs = json.loads(result.stdout)
              cutoff = datetime.now() - timedelta(days=days)

              recent_runs = []
              for run in runs:
                  created_at = datetime.fromisoformat(run['createdAt'].replace('Z', '+00:00'))
                  if created_at > cutoff:
                      recent_runs.append(run)

              return recent_runs

          def calculate_duration_seconds(created_at, updated_at):
              """Calculate run duration in seconds"""
              try:
                  start = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                  end = datetime.fromisoformat(updated_at.replace('Z', '+00:00'))
                  return (end - start).total_seconds()
              except:
                  return 0

          # Workflows to analyze
          critical_workflows = [
              'ci.yml',
              'h5-accuracy-report.yml',
              'polybench-sim.yml',
              'cpi-comparison.yml',
              'matmul-calibration.yml',
              'polybench-segmented.yml',
              'h5-parallel-accuracy.yml'
          ]

          # Collect data for all workflows
          dashboard_data = {
              "generated_at": datetime.now().isoformat(),
              "period_days": 30,
              "workflows": {},
              "summary": {},
              "trends": {}
          }

          all_runs_data = []

          for workflow in critical_workflows:
              print(f"Analyzing {workflow}...")
              runs = get_workflow_runs(workflow, days=30)

              if not runs:
                  dashboard_data["workflows"][workflow] = {
                      "total_runs": 0,
                      "success_rate": 0.0,
                      "avg_duration": 0.0,
                      "failure_rate": 0.0
                  }
                  continue

              # Process runs data
              completed_runs = [r for r in runs if r['status'] == 'completed']
              successful_runs = [r for r in completed_runs if r['conclusion'] == 'success']
              failed_runs = [r for r in completed_runs if r['conclusion'] in ['failure', 'cancelled', 'timed_out']]
              timeout_runs = [r for r in completed_runs if r['conclusion'] == 'timed_out']

              # Calculate metrics
              success_rate = len(successful_runs) / len(completed_runs) if completed_runs else 0.0
              failure_rate = len(failed_runs) / len(completed_runs) if completed_runs else 0.0

              # Duration analysis
              durations = [calculate_duration_seconds(r['createdAt'], r['updatedAt']) for r in completed_runs]
              avg_duration = np.mean(durations) if durations else 0.0

              dashboard_data["workflows"][workflow] = {
                  "total_runs": len(runs),
                  "completed_runs": len(completed_runs),
                  "successful_runs": len(successful_runs),
                  "failed_runs": len(failed_runs),
                  "timeout_runs": len(timeout_runs),
                  "success_rate": success_rate,
                  "failure_rate": failure_rate,
                  "avg_duration_seconds": avg_duration,
                  "avg_duration_minutes": avg_duration / 60.0
              }

              # Add to aggregate data for trending
              for run in completed_runs:
                  duration = calculate_duration_seconds(run['createdAt'], run['updatedAt'])
                  all_runs_data.append({
                      'workflow': workflow,
                      'date': datetime.fromisoformat(run['createdAt'].replace('Z', '+00:00')).date(),
                      'status': run['conclusion'],
                      'duration': duration,
                      'success': 1 if run['conclusion'] == 'success' else 0
                  })

          # Calculate summary metrics
          total_runs = sum([w['total_runs'] for w in dashboard_data["workflows"].values()])
          total_completed = sum([w['completed_runs'] for w in dashboard_data["workflows"].values()])
          total_successful = sum([w['successful_runs'] for w in dashboard_data["workflows"].values()])
          total_failed = sum([w['failed_runs'] for w in dashboard_data["workflows"].values()])

          dashboard_data["summary"] = {
              "total_runs": total_runs,
              "total_completed": total_completed,
              "total_successful": total_successful,
              "total_failed": total_failed,
              "overall_success_rate": total_successful / total_completed if total_completed > 0 else 0.0,
              "overall_failure_rate": total_failed / total_completed if total_completed > 0 else 0.0,
              "avg_daily_runs": total_runs / 30.0
          }

          # Generate visualizations
          print("Generating CI metrics dashboard...")

          # 1. Success Rate by Workflow (Bar chart)
          workflows_list = list(dashboard_data["workflows"].keys())
          success_rates = [dashboard_data["workflows"][w]["success_rate"] * 100 for w in workflows_list]

          fig = make_subplots(
              rows=2, cols=2,
              subplot_titles=('Success Rate by Workflow', 'Average Duration by Workflow',
                             'Daily Success Rate Trend', 'Workflow Run Volume'),
              specs=[[{"type": "bar"}, {"type": "bar"}],
                     [{"type": "scatter"}, {"type": "bar"}]]
          )

          # Success rate chart
          fig.add_trace(
              go.Bar(x=workflows_list, y=success_rates, name="Success Rate (%)",
                     marker_color=['green' if sr >= 90 else 'orange' if sr >= 70 else 'red' for sr in success_rates]),
              row=1, col=1
          )

          # Duration chart
          durations = [dashboard_data["workflows"][w]["avg_duration_minutes"] for w in workflows_list]
          fig.add_trace(
              go.Bar(x=workflows_list, y=durations, name="Avg Duration (min)",
                     marker_color='blue'),
              row=1, col=2
          )

          # Daily trend analysis
          if all_runs_data:
              df = pd.DataFrame(all_runs_data)
              daily_success = df.groupby('date')['success'].mean() * 100
              dates = daily_success.index
              success_trend = daily_success.values

              fig.add_trace(
                  go.Scatter(x=dates, y=success_trend, mode='lines+markers',
                           name="Daily Success Rate (%)", line_color='green'),
                  row=2, col=1
              )

              # Volume chart
              daily_volume = df.groupby('date').size()
              fig.add_trace(
                  go.Bar(x=daily_volume.index, y=daily_volume.values,
                         name="Daily Runs", marker_color='purple'),
                  row=2, col=2
              )

          fig.update_layout(
              title=f"CI Health Dashboard - {datetime.now().strftime('%Y-%m-%d')}",
              showlegend=False,
              height=800
          )

          # Save dashboard
          fig.write_html("ci_dashboard.html")
          fig.write_image("ci_dashboard.png", width=1200, height=800)

          # Create detailed metrics table
          with open("ci_metrics_table.html", "w") as f:
              f.write("""<!DOCTYPE html>
          <html><head><title>CI Metrics Table</title>
          <style>
          table { border-collapse: collapse; width: 100%; }
          th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
          th { background-color: #f2f2f2; }
          .success { background-color: #d4edda; }
          .warning { background-color: #fff3cd; }
          .danger { background-color: #f8d7da; }
          </style></head><body>
          <h1>CI Metrics Detailed Table</h1>
          <table><tr>
          <th>Workflow</th>
          <th>Total Runs</th>
          <th>Success Rate</th>
          <th>Avg Duration</th>
          <th>Failures</th>
          <th>Timeouts</th>
          <th>Health Status</th>
          </tr>""")

              for workflow, data in dashboard_data["workflows"].items():
                  success_rate = data["success_rate"] * 100
                  status_class = "success" if success_rate >= 90 else "warning" if success_rate >= 70 else "danger"
                  health = "Healthy" if success_rate >= 90 else "Degraded" if success_rate >= 70 else "Critical"

                  f.write(f"""<tr class="{status_class}">
                  <td>{workflow}</td>
                  <td>{data["total_runs"]}</td>
                  <td>{success_rate:.1f}%</td>
                  <td>{data["avg_duration_minutes"]:.1f} min</td>
                  <td>{data["failed_runs"]}</td>
                  <td>{data["timeout_runs"]}</td>
                  <td>{health}</td>
                  </tr>""")

              f.write("</table></body></html>")

          # Save data
          with open("ci_dashboard_data.json", "w") as f:
              json.dump(dashboard_data, f, indent=2)

          print("Dashboard generation complete!")
          print(f"Overall CI Health: {dashboard_data['summary']['overall_success_rate']*100:.1f}% success rate")
          PYEOF

      - name: Generate CI health trends
        run: |
          python3 - <<'PYEOF'
          import json
          import matplotlib.pyplot as plt
          from datetime import datetime

          # Load dashboard data
          with open("ci_dashboard_data.json", "r") as f:
              data = json.load(f)

          # Generate simple matplotlib charts as backup
          workflows = list(data["workflows"].keys())
          success_rates = [data["workflows"][w]["success_rate"] * 100 for w in workflows]

          # Create figure with subplots
          fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

          # Success rates
          colors = ['green' if sr >= 90 else 'orange' if sr >= 70 else 'red' for sr in success_rates]
          ax1.bar(range(len(workflows)), success_rates, color=colors)
          ax1.set_title('Success Rate by Workflow (%)')
          ax1.set_xticks(range(len(workflows)))
          ax1.set_xticklabels([w.replace('.yml', '') for w in workflows], rotation=45)
          ax1.set_ylabel('Success Rate (%)')
          ax1.axhline(y=90, color='green', linestyle='--', alpha=0.7, label='Target (90%)')
          ax1.legend()

          # Duration analysis
          durations = [data["workflows"][w]["avg_duration_minutes"] for w in workflows]
          ax2.bar(range(len(workflows)), durations, color='blue')
          ax2.set_title('Average Duration by Workflow (minutes)')
          ax2.set_xticks(range(len(workflows)))
          ax2.set_xticklabels([w.replace('.yml', '') for w in workflows], rotation=45)
          ax2.set_ylabel('Duration (minutes)')

          # Failure counts
          failed_counts = [data["workflows"][w]["failed_runs"] for w in workflows]
          timeout_counts = [data["workflows"][w]["timeout_runs"] for w in workflows]

          x = range(len(workflows))
          ax3.bar(x, failed_counts, label='Failed', color='red', alpha=0.7)
          ax3.bar(x, timeout_counts, bottom=failed_counts, label='Timeouts', color='darkred', alpha=0.7)
          ax3.set_title('Failure Breakdown by Workflow')
          ax3.set_xticks(x)
          ax3.set_xticklabels([w.replace('.yml', '') for w in workflows], rotation=45)
          ax3.set_ylabel('Failure Count')
          ax3.legend()

          # Summary metrics
          summary_labels = ['Total Runs', 'Successful', 'Failed']
          summary_values = [
              data["summary"]["total_runs"],
              data["summary"]["total_successful"],
              data["summary"]["total_failed"]
          ]
          ax4.pie(summary_values[1:], labels=['Successful', 'Failed'], autopct='%1.1f%%',
                  colors=['green', 'red'], startangle=90)
          ax4.set_title(f'Overall CI Health\n(30-day period)')

          plt.tight_layout()
          plt.savefig('ci_metrics_charts.png', dpi=300, bbox_inches='tight')
          plt.savefig('ci_metrics_charts.pdf', bbox_inches='tight')

          print("Additional charts generated successfully!")
          PYEOF

      - name: Create CI performance report
        run: |
          cat > ci_performance_report.md << 'MEOF'
          # CI Infrastructure Performance Report

          **Generated:** $(date -u)
          **Period:** Last 30 days
          **Dashboard Version:** Phase 3 CI Hardening Implementation

          ## Executive Summary

          MEOF

          # Add dynamic content to report
          python3 - <<'PYEOF'
          import json

          with open("ci_dashboard_data.json", "r") as f:
              data = json.load(f)

          summary = data["summary"]

          with open("ci_performance_report.md", "a") as f:
              f.write(f"""
          - **Overall Success Rate:** {summary['overall_success_rate']*100:.1f}%
          - **Total Runs Analyzed:** {summary['total_runs']}
          - **Average Daily Runs:** {summary['avg_daily_runs']:.1f}
          - **Total Failures:** {summary['total_failed']}

          ## Workflow Health Status

          | Workflow | Success Rate | Avg Duration | Status |
          |----------|-------------|--------------|---------|
          """)

              for workflow, metrics in data["workflows"].items():
                  success_rate = metrics["success_rate"] * 100
                  duration = metrics["avg_duration_minutes"]

                  if success_rate >= 90:
                      status = "‚úÖ Healthy"
                  elif success_rate >= 70:
                      status = "‚ö†Ô∏è Degraded"
                  else:
                      status = "‚ùå Critical"

                  f.write(f"| {workflow} | {success_rate:.1f}% | {duration:.1f}m | {status} |\n")

              f.write(f"""
          ## CI Hardening Implementation Status

          ### ‚úÖ Phase 1: Immediate Stability
          - Extended timeouts implemented across all workflows
          - H5 Accuracy Report: 60m ‚Üí 90m timeout
          - PolyBench Simulation: 20m ‚Üí 35m timeout
          - All core CI jobs have explicit timeout configurations

          ### ‚úÖ Phase 2: Performance Optimization
          - Parallel execution framework deployed
          - Test segmentation architecture implemented
          - PolyBench tests split into 3 groups for reliability
          - Multi-runner parallel H5 accuracy testing available

          ### ‚úÖ Phase 3: Long-term Resilience
          - CI health monitoring dashboard operational
          - Daily metrics collection and trending
          - Automated performance tracking
          - Silent failure detection mechanisms

          ## Recommendations

          """)

              # Add recommendations based on data
              critical_workflows = [w for w, m in data["workflows"].items() if m["success_rate"] < 0.7]
              degraded_workflows = [w for w, m in data["workflows"].items() if 0.7 <= m["success_rate"] < 0.9]

              if critical_workflows:
                  f.write("### ‚ö†Ô∏è Critical Issues\n")
                  for workflow in critical_workflows:
                      f.write(f"- **{workflow}**: Success rate below 70%, requires immediate attention\n")
                  f.write("\n")

              if degraded_workflows:
                  f.write("### üìà Performance Improvements\n")
                  for workflow in degraded_workflows:
                      f.write(f"- **{workflow}**: Success rate below 90%, consider optimization\n")
                  f.write("\n")

              if not critical_workflows and len(degraded_workflows) <= 1:
                  f.write("### ‚úÖ System Health\n")
                  f.write("CI infrastructure is performing well with no critical issues detected.\n")
                  f.write("Continue monitoring and consider proactive optimizations.\n\n")

              f.write("""
          ## Dashboard Artifacts

          - **Interactive Dashboard:** `ci_dashboard.html`
          - **Metrics Charts:** `ci_metrics_charts.png`
          - **Performance Data:** `ci_dashboard_data.json`
          - **Detailed Table:** `ci_metrics_table.html`

          ---
          *Report generated by CI Metrics Dashboard workflow - Phase 3 CI Infrastructure Hardening*
          """)

          print("Performance report generated successfully!")
          PYEOF

      - name: Upload comprehensive dashboard artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ci-dashboard-complete
          path: |
            ci_dashboard.html
            ci_dashboard.png
            ci_metrics_charts.png
            ci_metrics_charts.pdf
            ci_metrics_table.html
            ci_dashboard_data.json
            ci_performance_report.md
          retention-days: 90

      - name: Post dashboard summary
        if: always()
        run: |
          echo "## CI Metrics Dashboard Generated" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f ci_dashboard_data.json ]; then
            python3 -c "
          import json
          d = json.load(open('ci_dashboard_data.json'))
          s = d['summary']
          print(f\"**Overall CI Health:** {s['overall_success_rate']*100:.1f}% success rate\")
          print(f\"**Total Runs (30d):** {s['total_runs']}\")
          print(f\"**Average Daily Runs:** {s['avg_daily_runs']:.1f}\")
          print(f\"**Total Failures:** {s['total_failed']}\")
          print()

          critical = [w for w, m in d['workflows'].items() if m['success_rate'] < 0.7]
          degraded = [w for w, m in d['workflows'].items() if 0.7 <= m['success_rate'] < 0.9]

          if critical:
              print('### ‚ö†Ô∏è Critical Workflows (< 70%):')
              for w in critical:
                  print(f'- {w}: {d[\"workflows\"][w][\"success_rate\"]*100:.1f}%')

          if degraded:
              print('### üìä Degraded Workflows (70-90%):')
              for w in degraded:
                  print(f'- {w}: {d[\"workflows\"][w][\"success_rate\"]*100:.1f}%')

          if not critical and len(degraded) <= 1:
              print('### ‚úÖ All workflows performing well!')

          print()
          print('üìä [View Interactive Dashboard](artifacts/ci-dashboard-complete/ci_dashboard.html)')
          print('üìà [Download Performance Report](artifacts/ci-dashboard-complete/ci_performance_report.md)')
          " >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå Dashboard generation failed." >> $GITHUB_STEP_SUMMARY
          fi